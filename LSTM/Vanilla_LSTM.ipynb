{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla LSTM implementation from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is J.K. Rowling's Harry Potter and the Philosopher's Stone. \n",
    "\n",
    "I chose this text as the characteristic context and semantic structures present in the abundant dialogue, will help with evaluating the quality of results (also a huge HP fan...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Downloads/HP1.txt').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 442744 characters, 54 unique\n"
     ]
    }
   ],
   "source": [
    "chars = set(data)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print('data has %d characters, %d unique' % (len(data), vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionaries for mapping chars to ints and vice versa\n",
    "char_to_idx = {w: i for i,w in enumerate(chars)}\n",
    "idx_to_char = {i: w for i,w in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, char_to_idx, idx_to_char, vocab_size, num_hidden_units=100, \n",
    "                 seq_len=25, epochs=100, learning_rate=0.002, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Implementation of simple character-level LSTM using Numpy\n",
    "        \"\"\"\n",
    "        self.char_to_idx = char_to_idx  # dictionary that maps characters in the vocabulary to an index\n",
    "        self.idx_to_char = idx_to_char  # dictionary that maps indices to unique characters in the vocabulary\n",
    "        \n",
    "        self.vocab_size = vocab_size # number of unique characters in the training data\n",
    "        self.n_h = num_hidden_units # desirable number of units in the hidden layer\n",
    "        self.seq_len = seq_len # number of characters that will be fed to the LSTM in each batch (also number of time steps)\n",
    "        \n",
    "        self.epochs = epochs # number of training iterations\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1 # 1st momentum parameter\n",
    "        self.beta2 = beta2 # 2nd momentum parameter\n",
    "    \n",
    "        #---INITIALISE WEIGHTS AND BIASES---\n",
    "        self.params = {}\n",
    "\n",
    "        # forget gate\n",
    "        self.params[\"Wf\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * \\\n",
    "                            (1.0/np.sqrt(self.vocab_size + self.n_h))\n",
    "        self.params[\"bf\"] = np.ones((self.n_h,1))\n",
    "\n",
    "        # input gate\n",
    "        self.params[\"Wi\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * \\\n",
    "                            (1.0/np.sqrt(self.vocab_size + self.n_h))\n",
    "        self.params[\"bi\"] = np.zeros((self.n_h,1))\n",
    "\n",
    "        # cell gate\n",
    "        self.params[\"Wc\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * \\\n",
    "                            (1.0/np.sqrt(self.vocab_size + self.n_h))\n",
    "        self.params[\"bc\"] = np.zeros((self.n_h,1))\n",
    "\n",
    "        # output gate\n",
    "        self.params[\"Wo\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * \\\n",
    "                            (1.0/np.sqrt(self.vocab_size + self.n_h))\n",
    "        self.params[\"bo\"] = np.zeros((self.n_h ,1))\n",
    "\n",
    "        # output\n",
    "        self.params[\"Wv\"] = np.random.randn(self.vocab_size, self.n_h) * \\\n",
    "                            (1.0/np.sqrt(self.vocab_size))\n",
    "        self.params[\"bv\"] = np.zeros((self.vocab_size ,1))\n",
    "\n",
    "        #---INITIALISE GRADIENTS AND ADAM PARAMETERS---\n",
    "        self.grads = {}\n",
    "        self.adam_params = {}\n",
    "\n",
    "        for key in self.params:\n",
    "            self.grads[\"d\"+key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"m\"+key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"v\"+key] = np.zeros_like(self.params[key])\n",
    "            \n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab_size) * self.seq_len  # smoothing out loss as batch SGD is noisy\n",
    "        return\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Smoothes out values in the range of [0,1]\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def dsigmoid(self, y):\n",
    "        \"\"\"\n",
    "        Computes the gradient of sigmoid(x), but takes as an input\n",
    "        y = sigmoid(x) instead of x\n",
    "        \"\"\"\n",
    "        return y * (1 - y)\n",
    "\n",
    "    def dtanh(self, y):\n",
    "        \"\"\"\n",
    "        Computes the gradient of tanh(x), but takes as an input\n",
    "        y = tanh(x) instead of x\n",
    "        \"\"\"\n",
    "        return 1 - y**2\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Normalizes output into a probability distribution\n",
    "        \"\"\"\n",
    "        e_x = np.exp(x - np.max(x)) # max value is subtracted for numerical stability\n",
    "        return e_x / np.sum(e_x)\n",
    "    \n",
    "    def clip_grads(self):\n",
    "        \"\"\"\n",
    "        Limits the magnitude of gradients to avoid exploding gradients\n",
    "        \"\"\"\n",
    "        for key in self.grads:\n",
    "            np.clip(self.grads[key], -5, 5, out=self.grads[key])\n",
    "        return\n",
    "    \n",
    "    def reset_grads(self):\n",
    "        \"\"\"\n",
    "        Resets gradients to zero before each backpropagation\n",
    "        \"\"\"\n",
    "        for key in self.grads:\n",
    "            self.grads[key].fill(0)\n",
    "        return\n",
    "            \n",
    "    def update_params(self, batch_num):\n",
    "        \"\"\"\n",
    "        Updates parameters with Adam\n",
    "        \"\"\"     \n",
    "        for key in self.params:\n",
    "            self.adam_params[\"m\"+key] = self.adam_params[\"m\"+key] * self.beta1 + \\\n",
    "                                        (1 - self.beta1) * self.grads[\"d\"+key]\n",
    "            self.adam_params[\"v\"+key] = self.adam_params[\"v\"+key] * self.beta2 + \\\n",
    "                                        (1 - self.beta2) * self.grads[\"d\"+key]**2\n",
    "\n",
    "            m_correlated = self.adam_params[\"m\" + key] / (1 - self.beta1**batch_num)\n",
    "            v_correlated = self.adam_params[\"v\" + key] / (1 - self.beta2**batch_num) \n",
    "            # add small value to denominator for numerical stability\n",
    "            self.params[key] -= self.lr * m_correlated / (np.sqrt(v_correlated + 1e-8)) \n",
    "        return\n",
    "            \n",
    "    def sample(self, h_prev, c_prev, first_char_idx, sentence_length):\n",
    "        \"\"\"\n",
    "        Outputs a sample sequence from the model\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[first_char_idx] = 1\n",
    "        s = \"\"\n",
    "\n",
    "        h = h_prev\n",
    "        c = c_prev\n",
    "\n",
    "        for t in range(sentence_length):\n",
    "            y_pred, _, h, _, c, _, _, _, _ = self.forward_step(x, h, c)        \n",
    "\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            char = self.idx_to_char[idx]\n",
    "            s += char\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for one time step\n",
    "        \"\"\"\n",
    "        z = np.row_stack((h_prev, x))\n",
    "\n",
    "        f = self.sigmoid(np.dot(self.params[\"Wf\"], z) + self.params[\"bf\"])\n",
    "        i = self.sigmoid(np.dot(self.params[\"Wi\"], z) + self.params[\"bi\"])\n",
    "        c_bar = np.tanh(np.dot(self.params[\"Wc\"], z) + self.params[\"bc\"])\n",
    "\n",
    "        c = f * c_prev + i * c_bar\n",
    "        o = self.sigmoid(np.dot(self.params[\"Wo\"], z) + self.params[\"bo\"])\n",
    "        h = o * np.tanh(c)\n",
    "\n",
    "        v = np.dot(self.params[\"Wv\"], h) + self.params[\"bv\"]\n",
    "        y_pred = self.softmax(v)\n",
    "\n",
    "        return y_pred, v, h, o, c, c_bar, i, f, z\n",
    "    \n",
    "    \n",
    "    def backward_step(self, y, y_pred, dh_next, dc_next, c_prev, z, f, \n",
    "                            i, c_bar, c, o, h):\n",
    "        \"\"\"\n",
    "        Implements the backward propagation for one time step\n",
    "        \"\"\"\n",
    "        dv = np.copy(y_pred)\n",
    "        dv[y] -= 1\n",
    "\n",
    "        self.grads[\"dWv\"] += np.dot(dv, h.T)\n",
    "        self.grads[\"dbv\"] += dv\n",
    "\n",
    "        dh = np.dot(self.params[\"Wv\"].T, dv)\n",
    "        dh += dh_next\n",
    "\n",
    "        do = dh * np.tanh(c)\n",
    "        do = self.dsigmoid(o) * do\n",
    "        self.grads[\"dWo\"] += np.dot(do, z.T)\n",
    "        self.grads[\"dbo\"] += do\n",
    "\n",
    "        dc = np.copy(dc_next)\n",
    "        dc += dh * o * self.dtanh(np.tanh(c))\n",
    "\n",
    "        dc_bar = dc * i\n",
    "        dc_bar = self.dtanh(c_bar) * dc_bar\n",
    "        self.grads[\"dWc\"] += np.dot(dc_bar, z.T)\n",
    "        self.grads[\"dbc\"] += dc_bar\n",
    "\n",
    "        di = dc * c_bar\n",
    "        di = self.dsigmoid(i) * di \n",
    "        self.grads[\"dWi\"] += np.dot(di, z.T)\n",
    "        self.grads[\"dbi\"] += di\n",
    "\n",
    "        df = dc * c_prev\n",
    "        df = self.dsigmoid(f) * df\n",
    "        self.grads[\"dWf\"] += np.dot(df, z.T)\n",
    "        self.grads[\"dbf\"] += df\n",
    "\n",
    "        dz = (np.dot(self.params[\"Wf\"].T, df)\n",
    "             + np.dot(self.params[\"Wi\"].T, di)\n",
    "             + np.dot(self.params[\"Wc\"].T, dc_bar)\n",
    "             + np.dot(self.params[\"Wo\"].T, do))\n",
    "\n",
    "        dh_prev = dz[:self.n_h, :]\n",
    "        dc_prev = f * dc\n",
    "        return dh_prev, dc_prev\n",
    "    \n",
    "    \n",
    "    def forward_backward(self, x_batch, y_batch, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward and backward propagation for one batch\n",
    "        \"\"\"\n",
    "        x, z = {}, {}\n",
    "        f, i, c_bar, c, o = {}, {}, {}, {}, {}\n",
    "        y_pred, v, h = {}, {}, {}\n",
    "\n",
    "        # Values at t - 1\n",
    "        h[-1] = h_prev\n",
    "        c[-1] = c_prev\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(self.seq_len): #seq len\n",
    "            x[t] = np.zeros((self.vocab_size, 1))\n",
    "            x[t][x_batch[t]] = 1\n",
    "\n",
    "            y_pred[t], v[t], h[t], o[t], c[t], c_bar[t], i[t], f[t], z[t] = \\\n",
    "            self.forward_step(x[t], h[t-1], c[t-1])\n",
    "\n",
    "            loss += -np.log(y_pred[t][y_batch[t],0])\n",
    "\n",
    "        self.reset_grads()\n",
    "\n",
    "        dh_next = np.zeros_like(h[0])\n",
    "        dc_next = np.zeros_like(c[0])\n",
    "\n",
    "        for t in reversed(range(self.seq_len)):\n",
    "            dh_next, dc_next = self.backward_step(y_batch[t], y_pred[t], \n",
    "                                                  dh_next, dc_next, c[t-1], \n",
    "                                                  z[t], f[t], i[t], c_bar[t], \n",
    "                                                  c[t], o[t], h[t]) \n",
    "        return loss, h[self.seq_len-1], c[self.seq_len-1]\n",
    "    \n",
    "    \n",
    "    def gradient_check(self, x, y, h_prev, c_prev, num_checks=10, delta=1e-6):\n",
    "        \"\"\"\n",
    "        Checks the magnitude of gradients against expected approximate values \n",
    "        \"\"\"\n",
    "        print(\"**********************************\")\n",
    "        print(\"Gradient check...\\n\")\n",
    "\n",
    "        _, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "        grads_numerical = self.grads\n",
    "\n",
    "        for key in self.params: \n",
    "            print(\"---------\",key,\"---------\")\n",
    "            test = True\n",
    "\n",
    "            dims = self.params[key].shape\n",
    "            grad_numerical = 0\n",
    "            grad_analytical = 0\n",
    "\n",
    "            for _ in range(num_checks): #sample 10 neurons\n",
    "\n",
    "                idx = int(uniform(0, self.params[key].size))\n",
    "                old_val = self.params[key].flat[idx]\n",
    "\n",
    "                self.params[key].flat[idx] = old_val + delta\n",
    "                J_plus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val - delta\n",
    "                J_minus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val\n",
    "\n",
    "                grad_numerical += (J_plus-J_minus)/(2*delta)\n",
    "                grad_analytical += grads_numerical[\"d\"+key].flat[idx]            \n",
    "\n",
    "            grad_numerical /= num_checks\n",
    "            grad_analytical /= num_checks\n",
    "\n",
    "            rel_error = abs(grad_analytical-grad_numerical)/abs(grad_analytical+grad_numerical)\n",
    "\n",
    "            if rel_error > 1e-2:\n",
    "                if not (grad_analytical<1e-6 and grad_numerical<1e-6):\n",
    "                    test = False\n",
    "                    assert(test)\n",
    "\n",
    "            print('Approximate: \\t%e, Exact: \\t%e =>  Error: \\t%e'% (grad_numerical, grad_analytical, rel_error))\n",
    "        print(\"\\nTest successful!\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def train(self, X, verbose = True):\n",
    "        \"\"\"\n",
    "        Main method of the LSTM class where training takes place\n",
    "        \"\"\"\n",
    "        J = [] \n",
    "\n",
    "        num_batches = len(X) // self.seq_len\n",
    "        X_trimmed = X[: num_batches * self.seq_len]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            h_prev = np.zeros((self.n_h, 1))\n",
    "            c_prev = np.zeros((self.n_h, 1))\n",
    "\n",
    "            for j in range(0, len(X_trimmed) - self.seq_len, self.seq_len):  \n",
    "                # prepare batches\n",
    "                x_batch = [self.char_to_idx[ch] for ch in X_trimmed[j : j+self.seq_len]]\n",
    "                y_batch = [self.char_to_idx[ch] for ch in X_trimmed[j+1: j+self.seq_len+1]]                           \n",
    "                \n",
    "                # forward and back propagation\n",
    "                loss, h_prev, c_prev = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
    "                \n",
    "                # calculate loss\n",
    "                self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
    "                J.append(self.smooth_loss)\n",
    "                \n",
    "                # check gradients\n",
    "                if epoch == 0 and j == 0:\n",
    "                    self.gradient_check(x_batch, y_batch, h_prev, c_prev, \n",
    "                                        num_checks=10, delta=1e-7)\n",
    "                \n",
    "                # clip gradients\n",
    "                self.clip_grads()\n",
    "                \n",
    "                # update parameters\n",
    "                batch_num = epoch*self.epochs + j/self.seq_len + 1\n",
    "                self.update_params(batch_num)\n",
    "\n",
    "            # print out loss and sample output\n",
    "            if verbose:\n",
    "                print('Epoch:',epoch,'\\tBatch:',j,\"-\",j+self.seq_len,\n",
    "                      '\\tLoss:',self.smooth_loss)\n",
    "\n",
    "                s = self.sample(h_prev, c_prev, first_char_idx = x_batch[0], \n",
    "                                sentence_length = 250)\n",
    "                print(s,\"\\n\")\n",
    "\n",
    "        plt.plot([i for i in range(len(J))], J)\n",
    "        return J, self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(char_to_idx, idx_to_char, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Gradient check...\n",
      "\n",
      "--------- Wf ---------\n",
      "Approximate: \t-4.551026e-05, Exact: \t-4.551690e-05 =>  Error: \t7.289734e-05\n",
      "--------- bf ---------\n",
      "Approximate: \t-2.138229e-03, Exact: \t-2.138277e-03 =>  Error: \t1.120486e-05\n",
      "--------- Wi ---------\n",
      "Approximate: \t-8.654411e-06, Exact: \t-8.645099e-06 =>  Error: \t5.382425e-04\n",
      "--------- bi ---------\n",
      "Approximate: \t-7.351268e-03, Exact: \t-7.351303e-03 =>  Error: \t2.386090e-06\n",
      "--------- Wc ---------\n",
      "Approximate: \t-3.832348e-03, Exact: \t-3.832382e-03 =>  Error: \t4.518190e-06\n",
      "--------- bc ---------\n",
      "Approximate: \t-1.071315e-01, Exact: \t-1.071316e-01 =>  Error: \t3.376136e-07\n",
      "--------- Wo ---------\n",
      "Approximate: \t-3.801262e-04, Exact: \t-3.800889e-04 =>  Error: \t4.904505e-05\n",
      "--------- bo ---------\n",
      "Approximate: \t5.570300e-04, Exact: \t5.569927e-04 =>  Error: \t3.348348e-05\n",
      "--------- Wv ---------\n",
      "Approximate: \t2.859510e-02, Exact: \t2.859515e-02 =>  Error: \t8.341811e-07\n",
      "--------- bv ---------\n",
      "Approximate: \t-4.281368e-01, Exact: \t-4.281369e-01 =>  Error: \t7.080798e-08\n",
      "\n",
      "Test successful!\n",
      "**********************************\n",
      "\n",
      "Epoch: 0 \tBatch: 442675 - 442700 \tLoss: 39.52224164087512\n",
      " and miry him course!\" jutthere him, then surroon of right for a voeckeer could sushous, great duckled how quyiell, sury up and pisclen gurpling, nerre, not toids!' dusmings at them,, fell, there welleg anyo!\" she one moner other he scone, get lookin \n",
      "\n",
      "Epoch: 1 \tBatch: 442675 - 442700 \tLoss: 36.059501392102185\n",
      "s for gent,\" said harry who rosp up. wallosave pletford into them in there wasn't terking thing, or way at every. you parref, uneors of a mis! there books dublin face--- i wack hungle.\" \n",
      "\n",
      "shalled was run sursel, harry. \"she starply of burber usunents \n",
      "\n",
      "Epoch: 2 \tBatch: 442675 - 442700 \tLoss: 34.55283885244034\n",
      "s for, he was that be patter's working morning of passed at the tear through of his fifessoom. \n",
      "\n",
      "\"he pulled atsmath at the some chakight of the owles speal of drink -- and prefected and sporing your volliars,\" harry looked there fred is could never s \n",
      "\n",
      "Epoch: 3 \tBatch: 442675 - 442700 \tLoss: 33.70117778506361\n",
      "s are at harry's all if ludyenly, they become?\" \n",
      "\n",
      "\"it's took up to be quiethery.\" \n",
      "\n",
      "\"what dewnot to have all all funging up, humping,\" safuly lotter to the stopee there wanted use that say you, pearing at the facely, enough holdy calling the kind. du \n",
      "\n",
      "Epoch: 4 \tBatch: 442675 - 442700 \tLoss: 33.130348019399946\n",
      "s harry a moss and vincolr strange and off him. \n",
      "\n",
      "if adazem harry's memored steict. \n",
      "\n",
      "swush harry, are yes and a leady, what around of the table,\" said harry, \"now, went leaves full?\" \n",
      "\n",
      "\"ghunt uples lithers, not!\" he didn't must's harry points slemil \n",
      "\n",
      "Epoch: 5 \tBatch: 442675 - 442700 \tLoss: 32.729904769626025\n",
      " hall. \n",
      "\n",
      "\"my sort of wizard ineo. i's. they leet geings beaking rat as fortance outtaboded understarsute hang uncle af his ocking forgotter you. thenping at arm 'stare pointed of them up to a cindy for the wall, holding in the well, a oddy, bought ni \n",
      "\n",
      "Epoch: 6 \tBatch: 442675 - 442700 \tLoss: 32.39723100687455\n",
      "s homing you?' asked one looking yourte off fear his eyeh. \n",
      "\n",
      "ghet got secaled crip -- anot possomal younger were,\" said harry,\" said someone nothing booked, \"but him wegely, huttle looking furior; sive with shiff, you got away and my fer see got alon \n",
      "\n",
      "Epoch: 7 \tBatch: 442675 - 442700 \tLoss: 32.14339330587295\n",
      "s betting there we've got the conocks, full of mr. harry shocked. yeaked for got \"my next to the rollives off. \n",
      "\n",
      "\"idayed-wove of the potter, when he took ahake pair for its father pointed over all heep something. sill-hoor. \n",
      "\n",
      "\"i have their brother fr \n",
      "\n",
      "Epoch: 8 \tBatch: 442675 - 442700 \tLoss: 31.925508491500945\n",
      "s saxing sort of stood kean. one, alain, queeder-too still still undersignized him professor quirrell?\" said. \n",
      "\n",
      "harry made anything can wrended again, \"hagrid after this had pined its fulling snape's getting books decolay. they counted first, \"but he \n",
      "\n",
      "Epoch: 9 \tBatch: 442675 - 442700 \tLoss: 31.688587205451366\n",
      "s year, come with do he was potion, after silpredly happen. \n",
      "\n",
      "\"well, their air upins, hermione a. \n",
      "\n",
      "\"colladring all -- he had a sort of and for madce first.\" \n",
      "\n",
      "\"i being knows making yelling with the swear!\" \n",
      "\n",
      "\"she thinked themselves.... finnint smile \n",
      "\n",
      "Epoch: 10 \tBatch: 442675 - 442700 \tLoss: 31.526767139518203\n",
      "s bottered. \"so you mind to believe, \"i'll's, potter. \"not, but what happened dessed streilly.\" \n",
      "\n",
      "\"you don't, i'vli,\" hermione, well, needed them good, i really beht earet. \n",
      "\n",
      "even then his wayliza, but they josted a bad anyshipped and these will ieni \n",
      "\n",
      "Epoch: 11 \tBatch: 442675 - 442700 \tLoss: 31.420857225561097\n",
      "s.\" \n",
      "\n",
      "\"but have and much staring to clate; catally, let still so school?\" hermione, at repaying the watched, or be qutet last again gave a pointed.\" \n",
      "\n",
      "and it.\" \n",
      "\n",
      "he had found her mount.\" \n",
      "\n",
      "he always seemy over him less becams, that's for us. it she w \n",
      "\n",
      "Epoch: 12 \tBatch: 442675 - 442700 \tLoss: 31.3183331680126\n",
      "s two for for peeverne, with harry's good yourself, my down at ron and hermione, \"then and a dumbledore's seconds, good at him, we seemed to his real.\" \n",
      "\n",
      "he have to be you face... \"yeas giverthing frostrons and warned somate ttreeply-for quidditch ro \n",
      "\n",
      "Epoch: 13 \tBatch: 442675 - 442700 \tLoss: 31.187323610271786\n",
      "s smunding a look a letting start!\" said uncle dang quite year, and freas thought in eact of off in to cready beating at him. \n",
      "\n",
      "harry, bravery,\" said harry. \n",
      "\n",
      "harry, \"dumbledore and hermione, well, puil up enough. fall. \"harry; some of second. \"harry \n",
      "\n",
      "Epoch: 14 \tBatch: 442675 - 442700 \tLoss: 31.087379305157935\n",
      "ll little of anamed sweak, you classe fourchible to harry. \"oh!\" it didn't lacknogally. \"but they've gloding about still latele. you have questions!\" \n",
      "\n",
      "a moment way. \n",
      "\n",
      "for you knowright, panirenshopped pointed and next notes under possots you hear -- \n",
      "\n",
      "Epoch: 15 \tBatch: 442675 - 442700 \tLoss: 30.96678769038568\n",
      "s so of a beard-lause... ahomed to gave the nimbus. it askreey.\" \n",
      "\n",
      "jurmed, then he'd just adgrap as his wind out curse and pulled the mirror, it's to go.\" \n",
      "\n",
      "\"oh, dumbledore. he gottencly... \n",
      "\n",
      "\"for mostant have keep on: its, i'm going to helle's a bea \n",
      "\n",
      "Epoch: 16 \tBatch: 442675 - 442700 \tLoss: 30.95672256711854\n",
      "s beardwach!\" ron snapped a ages; there else they spat next mosting to killrill or heep and the bearrant you you line, well lifted to making pabbled it to purmlrier.\" \n",
      "\n",
      "it dirkice, teering into first knows couldn't batly, harry looked nott morning, a \n",
      "\n",
      "Epoch: 17 \tBatch: 442675 - 442700 \tLoss: 30.867286294120536\n",
      "ll hew any sweaty open!\" \n",
      "\n",
      "\"you.\" \n",
      "\n",
      "\"boon, he took a dark peases was seeker. \n",
      "\n",
      "any not a beator. \"malfoy! harry, hermione storeq one to themsed fer the time, he had to stoked only, under here of go formes a hundred as he happed, neared to hogies and  \n",
      "\n",
      "Epoch: 18 \tBatch: 442675 - 442700 \tLoss: 30.833835941231083\n",
      "d much to all seemy out of with harry scarled, harry!\" \n",
      "\n",
      "\"look award across the fawer curse in all they would you as athings to the magive,\" \n",
      "\n",
      "hagrid thought's lofe... \n",
      "\n",
      "something looking out of hig anything in their high,\" carry, whisting suddenly,  \n",
      "\n",
      "Epoch: 19 \tBatch: 442675 - 442700 \tLoss: 30.74055442900285\n",
      "r other lefted to ct for in gryffindor knocked at the whole as i  isn't looking ait, seemed up on!... not would, it course, he abth out of a vample. they smiled.\" \n",
      "\n",
      "\"\"it -- potter. \n",
      "\n",
      "\"quittle in, it had the striedly. \n",
      "\n",
      "\"well, my just bettered so what \n",
      "\n",
      "Epoch: 20 \tBatch: 442675 - 442700 \tLoss: 30.67426637003957\n",
      "s pumpkee and barking to last thought you arguintent. it was nose you floarding ron, who have that quiet bad terrarly rolled master; you, neville's smiling sort of themselved and will hard --\" \n",
      "\n",
      "\"yes, harry stepped up to award again, they cleak, peer \n",
      "\n",
      "Epoch: 21 \tBatch: 442675 - 442700 \tLoss: 30.672310298716297\n",
      "s guard. \"another bed again in them was thin, you're going to ron, their six it sunped up to uded for a gettingles at the moans and they spreast and petring lowed to our familious... you've got hagrid an' it's already be up or about last-rout, eville \n",
      "\n",
      "Epoch: 22 \tBatch: 442675 - 442700 \tLoss: 30.668462065042494\n",
      "d worrwed yeh, of cournts to the mirror. i son expected fortrater than blong at homework, as though on the teachering a troll in harry. you serioat!\" \n",
      "\n",
      "who's strangely, i adnt in comfeted worst! i printu,\" she was womean! being?\" said matching her fe \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \tBatch: 442675 - 442700 \tLoss: 30.64071084560715\n",
      "s meft. \n",
      "\n",
      "yes....\" \n",
      "\n",
      "he puppes asleep. \n",
      "\n",
      "\"he see a dreamed beator, malfoy.! as you be before hands, it was a pain's after malfoy to ask playing breath, their back through herkiolt toward the hutwantion long out that much was trying aloty, i hand to c \n",
      "\n",
      "Epoch: 24 \tBatch: 442675 - 442700 \tLoss: 30.602526163975433\n",
      "s oldgeor! wands. snape. the bead: these pain and belt. \n",
      "\n",
      "then sair umbrellow story. \n",
      "\n",
      "\"no, all, climbed blacknor of ravenclaw goald between,\" he spacked. minus exactly of points, stammed opened....\" harry had hearder; there dad, it was a hundred tha \n",
      "\n",
      "Epoch: 25 \tBatch: 442675 - 442700 \tLoss: 30.524278867585497\n",
      "dns head start she was off looner for inside first didn't bebait of they asked at only course must've mateod. sped meruy. \"hore only; voldemorst -- a standing off.\" \n",
      "\n",
      "neville need you?\" \n",
      "\n",
      "\"you poing lyoug,\" said, nothing as it was there.... \n",
      "\n",
      "\"hermio \n",
      "\n",
      "Epoch: 26 \tBatch: 442675 - 442700 \tLoss: 30.520636328965438\n",
      "s sormious all them stand outside harry was the true? became hungrid like after your hands and tell nersoring.\" going to round -- my humbre.... she was on his more through them already kitwardly, though to talking to the hoor,\" yearn the keoply wam,  \n",
      "\n",
      "Epoch: 27 \tBatch: 442675 - 442700 \tLoss: 30.563342259002976\n",
      "d seemed to be going to mistant of an everything at the ceaslemening out if cream all twins. oh, thought, and harry felt hoawher sate,\" ron! hermione's his enent old most icible, and suddenly letters realtite was a cauldring with for not getting in a \n",
      "\n",
      "Epoch: 28 \tBatch: 442675 - 442700 \tLoss: 30.486152227307116\n",
      "s way to looking at hogwarts more given of braviblings only everything, lee. but get off into it, well, there had i gone! d out, even we afterwand he looked and the black, years being stumbled to take me open peporicletlunds of much more was a siller \n",
      "\n",
      "Epoch: 29 \tBatch: 442675 - 442700 \tLoss: 30.415508991845957\n",
      "s home, ourse? you stood only or that come young up at its bot.... \n",
      "\n",
      "\"come on, imagine; and seventy year than, is were pate thing didn't want. \n",
      "\n",
      "\"we might catch them looking -- havin, because forgive some nighttly mouthed down, it sleeping slam unsta \n",
      "\n",
      "Epoch: 30 \tBatch: 442675 - 442700 \tLoss: 30.400912838736375\n",
      "s house --\" \n",
      "\n",
      "\"your present as there had a next toid and this. it wasn't got almost handifred. \"combled to his green, he started-busiar for yer friesd --' dumbledore looking a day and startly.., the right for the slytherins had to cale turned. most c \n",
      "\n",
      "Epoch: 31 \tBatch: 442675 - 442700 \tLoss: 30.335940327956276\n",
      "s father, then cross hermione was best.\" \n",
      "\n",
      "there friendrins and she had decided down them. so risks up, gryffindor, alfhing -- four bane them, ter viet in its breath,\" said gensy.\" she sprunger surprise safeare be the stoy, trying to hagriling to get \n",
      "\n",
      "Epoch: 32 \tBatch: 442675 - 442700 \tLoss: 30.391779740484864\n",
      "s tower, notsed a side sickled -- who couldn't slytunn you stand, the muggle bulle looking face, was gamadra and trouble.\" \n",
      "\n",
      "harry ased at then her?\" harry made supposed to way ron, and hermione lyind gringer, away... putsy in three of only there, de \n",
      "\n",
      "Epoch: 33 \tBatch: 442675 - 442700 \tLoss: 30.392307651277353\n",
      "dly, weresall-sopway, got and peered at once. who lost...\" \n",
      "\n",
      "\"yeah burried the treachers and its more -- these delievenc.\" \n",
      "\n",
      "something he wouldn't spraved outside the crosced, one, it espuned, pept up the relirge, and stattering him, if your father's \n",
      "\n",
      "Epoch: 34 \tBatch: 442675 - 442700 \tLoss: 30.316597722605906\n",
      "s. \n",
      "\n",
      "harry had tell. \n",
      "\n",
      "\" he wasn't one were,\" said hermione was all them! you let alpott of note, past ron tapped has use when he can't a rob the grarmly, pointing in a proculably, go; harry's -- but said then an' in any!\" 'olle goen out occhmbles. h \n",
      "\n",
      "Epoch: 35 \tBatch: 442675 - 442700 \tLoss: 30.28657738109535\n",
      "s owlered in the row.. and off. \n",
      "\n",
      "\"that in the snight. \n",
      "\n",
      "a questisnals knokhed and a body,\" she showed harry, told out point of size full, peeves, potter.\" \n",
      "\n",
      "a deepe - well, talking their very she weasley's miss at please. he want potter, pinned a ch \n",
      "\n",
      "Epoch: 36 \tBatch: 442675 - 442700 \tLoss: 30.200168082641436\n",
      "s here into a still friends, very lower's wrong. \n",
      "\n",
      "\"oh, he'd foundrient. you or-rooy.\" \n",
      "\n",
      "and harry and hurried. \"well, were so you took much at all off your arrmist professor shape allowed secricily, by a bean, but there only call unoring at the hurt \n",
      "\n",
      "Epoch: 37 \tBatch: 442675 - 442700 \tLoss: 30.20686475596829\n",
      "s still the book, forgive and strongly had sure youkay of them, you noticered into his horroratly down?\" she have spraveling, they stronght looking for they musticy.\" \n",
      "\n",
      "you'll be going wild-harfs subroty!\" \n",
      "\n",
      "\"mar. harry, who could. \n",
      "\n",
      "\"me....\" \n",
      "\n",
      "\"do y \n",
      "\n",
      "Epoch: 38 \tBatch: 442675 - 442700 \tLoss: 30.203671279482666\n",
      "s werewor preses for braveling what into numbe what the faces he stared -- are strontich. \n",
      "\n",
      "the stone! not centaur.\" \n",
      "\n",
      "all ron didn't better frient headss-ly. \n",
      "\n",
      "\"i'm, he got doing quete bottlent,\" said hermione than the ways yellinat this.\" \n",
      "\n",
      "\"holdin \n",
      "\n",
      "Epoch: 39 \tBatch: 442675 - 442700 \tLoss: 30.220975097215717\n",
      "s before oh of well, professor sprodued it his crates time to come to dumbledore's pass slet's where i not be killed, but in one than that unusual..\" \n",
      "\n",
      "\"so, they knowing , where they tried to give wizards. \n",
      "\n",
      "even sixty not her why hagrid,\" ron, mrs.  \n",
      "\n",
      "Epoch: 40 \tBatch: 442675 - 442700 \tLoss: 30.225834425433394\n",
      "s seek poising face jumpeat, stalt on the waiting ron and hermione. a loud tallive colla. \"i thought a stufe in for huma-en. sweesing a deal. his lams. ser, harry.\" \n",
      "\n",
      "look! once year.\" \n",
      "\n",
      "\"yes.brie, not they'd be her?\" as they stold at quirrell of beh \n",
      "\n",
      "Epoch: 41 \tBatch: 442675 - 442700 \tLoss: 30.201038540355402\n",
      "s greeod or i can. be faithace, neville's againy!\" ron sloped in the man. he'd suppoted! there's not many caves with harry's gool hermione thereful, here and your voices! that gryffindor told their fifty key, but but you know...\" lost it. then life a \n",
      "\n",
      "Epoch: 42 \tBatch: 442675 - 442700 \tLoss: 30.236068870936972\n",
      "s,\" said humsing, got a few ron? sixty fallen had he extil quieth of ages bean. \"so quirrell,\" ron, carefal, and me, ron, school without tape rof's robes. something?\" harry could hear they'll didn't hear's close?\" she colled him, all gryffindor was d \n",
      "\n",
      "Epoch: 43 \tBatch: 442675 - 442700 \tLoss: 30.2152079889651\n",
      "s betthentite through their merald,\" snarle he stood, nice, nor, hermione fulled; hermione and bone word much cheering on womacied, sip? my, malfoy, pepping out of the first more. a.\" \n",
      "\n",
      "he asked his treathe. \n",
      "\n",
      "\"notica!r at the prome...\" \n",
      "\n",
      "\"always fin \n",
      "\n",
      "Epoch: 44 \tBatch: 442675 - 442700 \tLoss: 30.1200609824303\n",
      "h mountainible started acrossisher. \n",
      "\n",
      "they allowed out, sib winged.\" \n",
      "\n",
      "\"soon. i call eatin' save up!\" \n",
      "\n",
      "tings very off, seventy, sip good?\" said maron, looking -- i didn't left the stone going on, and dumbledore and speining coldly, he'll unlyzed sud \n",
      "\n",
      "Epoch: 45 \tBatch: 442675 - 442700 \tLoss: 30.22509139326762\n",
      "s, too... i'll be --\" \n",
      "\n",
      "\"then, even thought i.\" \n",
      "\n",
      "he kept ron looked -- he didn't, as yeh're near thore wanted man, since off adaid,\" said harry almorched. \n",
      "\n",
      "they were than before white before have bather will save the unicorn. \n",
      "\n",
      "and around. but was  \n",
      "\n",
      "Epoch: 46 \tBatch: 442675 - 442700 \tLoss: 30.21096340678896\n",
      "s glad next time, but shaking in. \"them,\" said hurried? snape had been roam,\" ron sat been anxious mad!\" ron in them...\" \n",
      "\n",
      "say. \"oh, no an eithers toad-becid! snape confors forward robmy. it won't slid went.... professor me empty of examine pawnight  \n",
      "\n",
      "Epoch: 47 \tBatch: 442675 - 442700 \tLoss: 30.15416528356204\n",
      "s fircos from the good gangs, then noir's even snarched cherse, were the bin tight.\" \n",
      "\n",
      "\"he'rl him, however. \n",
      "\n",
      "\"don't you neeperful could 'counthely coldly... i've got still or a dursleys sit of the potter's ninger-landed -- and quietly, but of short. \n",
      "\n",
      "Epoch: 48 \tBatch: 442675 - 442700 \tLoss: 30.10523267323036\n",
      "s battle d-roy.\" \n",
      "\n",
      "make match in, and yeh sort all argue -- swilly. \n",
      "\n",
      "\"well, put and outside from the talking purple of his robes, they'll need i send me, harry!\" \n",
      "\n",
      "\"noxty hursling armchers, and over sincereal bit out of herrown pointing on traing to \n",
      "\n",
      "Epoch: 49 \tBatch: 442675 - 442700 \tLoss: 30.144032723134373\n",
      "s telling out of it gryffindors for you oans away frighten became but good into the place... neville in i between his mughteoses why didn't get proper's -- never got their dreamsicus. \n",
      "\n",
      "\"held into merst, \"it?\" said hand unicorn. \"always county- you m \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 \tBatch: 442675 - 442700 \tLoss: 30.059701399182533\n",
      "s way on bottering; highperace his loff; he hadn't yeh. a lot, said hutting all ron.\" \n",
      "\n",
      "\"all gloaking at potter. weasley, \"i'll catchtening a bearts! harry lookered for the standings?\" said roundly. \"they got to knoes cluth wher those my siyery, ron. \n",
      "\n",
      "Epoch: 51 \tBatch: 442675 - 442700 \tLoss: 30.023106043976\n",
      "s this thought always famous word of putting bothing, \"the rolled tonight.\" \n",
      "\n",
      "how their week... you've got you?. hard stones, and harry walked in a down racening that looking unside if, musambard.\" \n",
      "\n",
      "\"and settiminger happens -- prefects he norbert ba \n",
      "\n",
      "Epoch: 52 \tBatch: 442675 - 442700 \tLoss: 29.957123974258558\n",
      "s hermione scash and could snape better,\" said hermione with puting through thoid, went, saw for them. \n",
      "\n",
      "\"be three-werewor owctmous through it, harry, without quiet looking pointing sofe and measly. \n",
      "\n",
      "\"and not up to the portralle slytherin; in the co \n",
      "\n",
      "Epoch: 53 \tBatch: 442675 - 442700 \tLoss: 30.079871670730885\n",
      "s,\" said ron, not his intomage of mustached them, sidger going to bloking, felt in the pasting them.... you sparking for bit of real back, kitchingen quietly, \"howes keeper in wherking some other very from them when he was the wall than twelve house  \n",
      "\n",
      "Epoch: 54 \tBatch: 442675 - 442700 \tLoss: 29.996031468340156\n",
      "s there yet nicolas to cault mer, then heald with one for professor snape interreather of throughpa9'sline; when those,\" \n",
      "\n",
      "\"the there curtain. \n",
      "\n",
      "\"vilic' all lymerist amazement. his name?\" \n",
      "\n",
      "\"they could hear that could have 'em out of the deencer, i m \n",
      "\n",
      "Epoch: 55 \tBatch: 442675 - 442700 \tLoss: 29.920631581226168\n",
      "s deep stiflite, mering at all, ahar being get home these moved she must breath point so waiting guard. the guardust rright, you cail. \n",
      "\n",
      "\"first -- i not going almest just shoet, and start here!\" \n",
      "\n",
      "as when he have and and they explodions,\" said snape  \n",
      "\n",
      "Epoch: 56 \tBatch: 442675 - 442700 \tLoss: 29.959214316652254\n",
      "s, or very human -- playing,\" said ron, later. \n",
      "\n",
      "are are ready... then dumbledore ,\" harry even quidears,\" \n",
      "\n",
      "\"threling into they didn't set their. near's magic for them. all them finish, but all mstuand, the week quickly, feeling in his very fire out \n",
      "\n",
      "Epoch: 57 \tBatch: 442675 - 442700 \tLoss: 29.874039679041196\n",
      "s looking at the bishofors, where good by quirrell, d'yeh volie reddidden away. \n",
      "\n",
      "\"snape didn't, and they to mustah and two stop the states.\" \n",
      "\n",
      "apartably he'd have to find it over their old steal. \n",
      "\n",
      "\"then sevenus will statt norcy that afbeed out, whi \n",
      "\n",
      "Epoch: 58 \tBatch: 442675 - 442700 \tLoss: 29.869011881262214\n",
      "s,\" he said. he add back of quills. a table. the potions throary -- they thinzer-of-ly silentself that shaking normand and get out. it is around anyone in elesty next of harry as the dusto, four fothome place, but dumbledore. there were had raak, how \n",
      "\n",
      "Epoch: 59 \tBatch: 442675 - 442700 \tLoss: 29.893784402855054\n",
      "s the front curse...\" \n",
      "\n",
      "he shivered hurman, lie, and alwory -- they outlow -- hise through them in oness awablaging them -- toos, better what looked lyint to only worry. \n",
      "\n",
      "\"yever they scrueched, putting much in,\" said ron. \n",
      "\n",
      "\"bin up the black-lif you \n",
      "\n",
      "Epoch: 60 \tBatch: 442675 - 442700 \tLoss: 29.836353767139013\n",
      "s him not the rushed one, but we were practic the greatue?\" said harry. \n",
      "\n",
      "\"i hope it was a roarful's face, and then shadowling through the spot. we won -- their fear, he was took them boiling! a hormas trees floor and still had ever know -- i'll have \n",
      "\n",
      "Epoch: 61 \tBatch: 442675 - 442700 \tLoss: 29.86355187200044\n",
      "s next to him the air,\" said harry. harry get again, and made malfoys, threefiormate, i hanger in frogifest by them. he took a head of him, wizards frighed him.\" harry was said, ron, and until he kept his head. the bloodyos.\" \n",
      "\n",
      "\"were a cloak beancer. \n",
      "\n",
      "Epoch: 62 \tBatch: 442675 - 442700 \tLoss: 30.04212893274519\n",
      "s guarding allowed to your grounds fromous,\" said harry, pointing. \"buttered over yourself, but life, and walked in a large giance, and get us stee bad inside, behind them over the gable, toulardan?\" whispered him happents neville!\" she wale a head i \n",
      "\n",
      "Epoch: 63 \tBatch: 442675 - 442700 \tLoss: 29.95212086197659\n",
      "s, lay. then suddenly then, as he really really had no good rucked and george few point reached them little on next moment, now howesh. \n",
      "\n",
      "\"bottor faces them deads, all invisibiling, i sless, harry. \n",
      "\n",
      "\"you won't euches the glades, out! hels. \n",
      "\n",
      "\"'er sa \n",
      "\n",
      "Epoch: 64 \tBatch: 442675 - 442700 \tLoss: 29.893658278304045\n",
      "s white if hag'id, good-bye the cloak of bottle sabow at all, they'll talk into if you hast me ask my hand outside age at the scraing ron, not come, something moving the right away. professor uprars with and hard were tre termiore,\" said hagrid profe \n",
      "\n",
      "Epoch: 65 \tBatch: 442675 - 442700 \tLoss: 29.825890238646625\n",
      "s or placling who there still as frienty looking roundoward -- and not the footcting him sir, tried to sensing tights and harry looked fulloors or wizard,. \"thanks so hedwars very so that harry and them, looking muggle, when you got to me, and come a \n",
      "\n",
      "Epoch: 66 \tBatch: 442675 - 442700 \tLoss: 29.881475151111815\n",
      "s. there's a gone to inly.\" \n",
      "\n",
      "\"sknig they always they knew whatored. i meaned into a lot out each other were pocking at use.\" \n",
      "\n",
      "\"breath, and team-\"'candering the stone!\" the chpoched arm!\" rof here of them. it wasn't ket -- you gets about them, then, \n",
      "\n",
      "Epoch: 67 \tBatch: 442675 - 442700 \tLoss: 29.87175803991218\n",
      "s smooth incredible, where you just know of the weeks salworself would stone pub outle and sure she slithed opens, \"see it skeple. ron noting brandy quieple...\" harry was these unusaved at the backwastay. \"have a hugeres, not,\" said ron had the groun \n",
      "\n",
      "Epoch: 68 \tBatch: 442675 - 442700 \tLoss: 29.814565583006615\n",
      "s uffoombled rounded. harry opened and i think it's young three-erning one stop much answered, ron.\" \n",
      "\n",
      "quirrell, unlefte in the stones for you, and helpening me gool's even fluffy saw things beasture, were quite quite come of baging hagrid agiase we' \n",
      "\n",
      "Epoch: 69 \tBatch: 442675 - 442700 \tLoss: 29.886076754459506\n",
      "s when they had done quidewly real!\" it said harry\" \n",
      "iffed they had not,\" said ron. \n",
      "\n",
      "\"bright boy shapped the very mean no,\" said harry as harry and hermione?\" said harry time after in... no of again, and his father and glowing as secrible and taped  \n",
      "\n",
      "Epoch: 70 \tBatch: 442675 - 442700 \tLoss: 29.733473183126783\n",
      "s. \n",
      "\n",
      "\"what could it look at my wizard of obder strunts us. \n",
      "\n",
      "theers, are you to get out: a smell again,\" said harry, too. \n",
      "\n",
      "he looked for the halt smell round green everyone and hermione's at his quilling to you mutter-nall now all ginny, the green.  \n",
      "\n",
      "Epoch: 71 \tBatch: 442675 - 442700 \tLoss: 29.817668058782836\n",
      "s moon, and lons up, right! should!\" hagrid smiled and belized now more than of people as ter herrost looking a left for force much and the great oming their great looking very nour life; not sitting and he found their moments; i beild working and sc \n",
      "\n",
      "Epoch: 72 \tBatch: 442675 - 442700 \tLoss: 29.84569957967296\n",
      "s here,\" harry and hermione had gone ahoce -batered off you goes and when ron,\" said me on they doess than them, he couldn't block house pludger right planted in harry's old time, his goble in hurried as silver took out land, we no volughing that she \n",
      "\n",
      "Epoch: 73 \tBatch: 442675 - 442700 \tLoss: 29.826742166551966\n",
      "s a binter points, the poorer, who even it green frighten, standing all more better an eyes your bedragedly. \"but she took a look in the lateh at once!\" \n",
      "\n",
      "spellal... \n",
      "\n",
      "you addry-fulling somethin' from to anyone was course -- snape says that shaking!  \n",
      "\n",
      "Epoch: 74 \tBatch: 442675 - 442700 \tLoss: 29.922634801022408\n",
      "s so them. a lot to him, suppored but professor.\" \n",
      "\n",
      "quirrell hardly quietled, suddenly look!s but at eating air's eable down the christmas of their bed.\" \n",
      "\n",
      "harry hadn't you -- joke applaying it in flamet, who was you, lion, suddenly flarled come that \n",
      "\n",
      "Epoch: 75 \tBatch: 442675 - 442700 \tLoss: 29.764972888494828\n",
      "s nowsord three-headed down it in the botter. all or his face. \n",
      "\n",
      "\"not station. \n",
      "\n",
      "\"wollem quickly. \"ahel' dryfomy had walked at a laggeming withod handle, then very exam me -- muttered, harry followed into around the whole the day. he had gone adventu \n",
      "\n",
      "Epoch: 76 \tBatch: 442675 - 442700 \tLoss: 29.889592543107526\n",
      "s you, one three-quarted, doinutfring; forgest spaped, up louble and harry. \n",
      "\n",
      "it started -- thought began, \"then tried to get professor hedwias,\" said madam hard to look at them in the while in a man. share tape. \n",
      "\n",
      "\"no... your forest, a pair. \"well.  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 \tBatch: 442675 - 442700 \tLoss: 29.783509386122343\n",
      "s two. \n",
      "\n",
      "\"lost magic flate pointing many arms in front of neville's more was one of professor quirrell's again, at the could meangsten very coan. one book, not themselves by ronan in second of owl, that would mirror of if you know, of course the midn \n",
      "\n",
      "Epoch: 78 \tBatch: 442675 - 442700 \tLoss: 29.705400794152965\n",
      "s down and very thicker. \n",
      "\n",
      "good wip his eyeore?\" \n",
      "\n",
      "the good will be at from with the stared, looked down to quierly, but they were already, toward three interes, great gather, because its been. his note. \n",
      "\n",
      "\"all ears quirrell's chets! \"we don't be a u \n",
      "\n",
      "Epoch: 79 \tBatch: 442675 - 442700 \tLoss: 29.71738501855036\n",
      "s face,\" said ron, through the sor. nearly bent you in hufflepuff.\" \n",
      "\n",
      "\"arguen... hogwart to -- for hutsly -- well,\" said ron, neither will healts; champior uncees. he got, and they're that cursly at then award a corridor silvly. \n",
      "\n",
      "\"but when you hadn' \n",
      "\n",
      "Epoch: 80 \tBatch: 442675 - 442700 \tLoss: 29.736478886889923\n",
      "s crept owl forward, poorers,\" fire, and against up, big, yourself first winged. he then he pointed in a muggle. hermione, not forgiverivey, thought he turned, and dear.\" \n",
      "\n",
      "\"you could. he found him nearer; they?\" he said. \n",
      "\n",
      "courine off harry garly, n \n",
      "\n",
      "Epoch: 81 \tBatch: 442675 - 442700 \tLoss: 29.789646839149416\n",
      "s learness outsally. \"now than the airs happened on womaging tower, snape shouldn't let out, still,\" said madnings and empty, people will have eating, and some shuffly. \"yeah, are yet as everythered you weren't as get us the graspering at them were g \n",
      "\n",
      "Epoch: 82 \tBatch: 442675 - 442700 \tLoss: 29.6722904457079\n",
      "s to under one chriugibout arrivered of you remember... you reveary sweat their oldrefes,\" said ron. \n",
      "\n",
      "\"now,\" said harry, professor.\" \n",
      "\n",
      "it was amoney, to next high to touch the board, across the way over. it was over...\" \n",
      "\n",
      "several off through him cal \n",
      "\n",
      "Epoch: 83 \tBatch: 442675 - 442700 \tLoss: 29.834988035289882\n",
      "s them saw the sweater, but two offory few mean, hurried forward, but i mean, there could feel pile of once told him, but which one to dumbledore,\" said harry and hermione, but thesereds ago,\" harry stared.\" \n",
      "\n",
      "\"well?\" said ron, firefors voice many or \n",
      "\n",
      "Epoch: 84 \tBatch: 442675 - 442700 \tLoss: 29.68787906114442\n",
      "s again....\" \n",
      "\n",
      "\"said we suddencard, and we are now, torned to him, that grunted, \"so and and started told quirrell's between the chander of mile, took an excead no coiny. while. \n",
      "\n",
      "\"was at ord. they had and he seemed to warned us took protatvion of ba \n",
      "\n",
      "Epoch: 85 \tBatch: 442675 - 442700 \tLoss: 29.685964192311733\n",
      "dlated hardly rumeas) so much.\" \n",
      "\n",
      "\"than a hugalond is rubbay. \n",
      "\n",
      "they begand to catch you,\" shele?\" \n",
      "\n",
      "\"i suppose, they saw each moment he didn't look through fluffy. \n",
      "\n",
      "\"find me to the faix of gases dropped the fell-yels. \"well, how for mine, suddenly  \n",
      "\n",
      "Epoch: 86 \tBatch: 442675 - 442700 \tLoss: 29.647810299016882\n",
      "s has noticing out her. see, you want second with mistagr, that's spell of became furious terrible paver towering gales relur! east-sensing, bottle quirrell told me on, \"truer of, but this vermilieg he rained the flute purple as thought, and come pur \n",
      "\n",
      "Epoch: 87 \tBatch: 442675 - 442700 \tLoss: 29.765201808998885\n",
      "s station in front, hermione had turned boy!\" \n",
      "\n",
      "\"were back. \n",
      "\n",
      "harry. at the points and at ron, because seem in starty rodger outsleed of ron, white began them wip xide question. very sorckanci againstmy, in a not!\" \n",
      "\n",
      "\"ah!\" \n",
      "\n",
      "he was almost a very and  \n",
      "\n",
      "Epoch: 88 \tBatch: 442675 - 442700 \tLoss: 29.676569403140913\n",
      "s they were sickle their hands, one stone remember and one ofe light. \n",
      "\n",
      "\"harry gets again.\" \n",
      "\n",
      "\"why gots to find you on stop! \"use it, that, it were able to kille. he waited. \n",
      "\n",
      "\"ture.\" \n",
      "\n",
      "they looked, there was a hands of- great stime leng to the. he p \n",
      "\n",
      "Epoch: 89 \tBatch: 442675 - 442700 \tLoss: 29.68259889222547\n",
      "s altifice looking grawn knew.\" \n",
      "\n",
      "whete voullen and hogwarts, we're sorry, the stone off it?\" he are some next more by that me stone couldn't seet i each oring, tailing behind the skying the old body kid back, wanted had been in him.\" \n",
      "\n",
      "not understan \n",
      "\n",
      "Epoch: 90 \tBatch: 442675 - 442700 \tLoss: 29.600464616353676\n",
      "s sop aroul, that's you if it's kired of mell, another face of makin something celeblus calm and sheetupe, to go to get us an a many of them territo o' a beiters couldn't, standing forest for manion were so that quite yes,\" said rout, peeves armily,  \n",
      "\n",
      "Epoch: 91 \tBatch: 442675 - 442700 \tLoss: 29.56109817504818\n",
      "s tors, three fide she hots... quiefle, admirment.\" \n",
      "\n",
      "she muttered call anly.\" \n",
      "\n",
      "not followers, sprapped them down to see they specially face were nervous. no money down draco our as all over the son, took him and called me the other face, his mosty  \n",
      "\n",
      "Epoch: 92 \tBatch: 442675 - 442700 \tLoss: 29.582704551601534\n",
      "s opened his feet its head; behind ron with my dragon seem his feetrarly. he leaving sevpulpise, very that i see.\" \n",
      "\n",
      "\"what saying to make that sive as my jacted to each gardly. \"stick.\" she shouled up. \n",
      "\n",
      "\"i've got one of the left behind what do ready \n",
      "\n",
      "Epoch: 93 \tBatch: 442675 - 442700 \tLoss: 29.54381371927505\n",
      "s year... purpy and three-quaffhed dead, the flame ajar and came more great next meant trerting back at the giody to the trouble ear. \n",
      "\n",
      "\"ron didn't take what was withering on them for eehoused together, twenty, really a huge table,\" said me levinmed  \n",
      "\n",
      "Epoch: 94 \tBatch: 442675 - 442700 \tLoss: 29.66696312538487\n",
      "s you was there's yet?\" harry didn't stop, where you well jord fewleed in a couldn't be. hermione was pointin' me to she thater thought yes, almost left. they could you points,\" snarled. \"they've just thought he began someone to mind and harry were l \n",
      "\n",
      "Epoch: 95 \tBatch: 442675 - 442700 \tLoss: 29.609316607997098\n",
      "s a rusten trur lookinge and got the bottle top read more huge going on really staffers. \n",
      "\n",
      "\"to the malform, were always heads in the library.\" said a station. potion,\" said harry. rive potter and vooder down as harry. \n",
      "\n",
      "but all powers and rassle. pet \n",
      "\n",
      "Epoch: 96 \tBatch: 442675 - 442700 \tLoss: 29.57890717222511\n",
      "s sometimes \"gryffindor smelly and peers. \n",
      "\n",
      "\"i mean, ! ,\" harry soured yester made large involled in my voice things gruring to candyeal look points this suspas and a damage hasn't do lages at big lade?\" said their name magic, so when they scared him \n",
      "\n",
      "Epoch: 97 \tBatch: 442675 - 442700 \tLoss: 29.610637861416134\n",
      "s to death you  so want them. \n",
      "\n",
      "\"i'll be thinking along them... hear, it couldn't over by. look your of us sort of thousand. \"we think straigh h-or confusing straining black bannite, looking for a few hagrid reached ron, but a going as though, they c \n",
      "\n",
      "Epoch: 98 \tBatch: 442675 - 442700 \tLoss: 29.63447432162191\n",
      "s.\" \n",
      "\n",
      "\"she best dumbledore would be boy. \n",
      "\n",
      "it was really worst point most quite, because he expect to she had someone fained at the leg really's. punious, they -- and managed to die.\" \n",
      "\n",
      "\"how guard here nearly a mixtufered, nace, \"i'll got a real very \n",
      "\n",
      "Epoch: 99 \tBatch: 442675 - 442700 \tLoss: 29.637054468190577\n",
      "s smell behind them in its eye that's someone pleased along of chess, harry,\" hermione weren co?\" grunned that grunned the door cold into the hard were, liming can do?\" \n",
      "\n",
      "\"shasty points hunding of rute, it was at eating back there. \n",
      "\n",
      "\"could?\"'re face \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "J, params = model.train(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
